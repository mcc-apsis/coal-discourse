{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "# import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "# preamble for jupyter notebook and django\n",
    "import django\n",
    "import platform\n",
    "from django.db.models import Q, F, Sum, Count, FloatField, Case, When\n",
    "\n",
    "if platform.node() == \"mcc-apsis\":\n",
    "    sys.path.append('/home/muef/tmv/BasicBrowser/')\n",
    "else:\n",
    "    # local paths\n",
    "    sys.path.append('/media/Data/MCC/tmv/BasicBrowser/')\n",
    "\n",
    "#sys.path.append('/home/galm/software/django/tmv/BasicBrowser/')\n",
    "os.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"BasicBrowser.settings\")\n",
    "django.setup()\n",
    "\n",
    "# import from appended path\n",
    "import parliament.models as pm\n",
    "from parliament.tasks import do_search, run_tm\n",
    "import cities.models as cmodels\n",
    "from django.contrib.auth.models import User\n",
    "import tmv_app.models as tm"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": 6,
>>>>>>> 2debe65e8c2925068ff9da23ad136e66679c763f
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253\n"
     ]
    }
   ],
   "source": [
    "docs = pm.Document.objects.filter(parlperiod__n=17, text_source=\"from https://www.bundestag.de/service/opendata \"\n",
    "                                              +\"(scans of pdfs with xml metadata)\" )\n",
    "print(docs.count())"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 7,
>>>>>>> 2debe65e8c2925068ff9da23ad136e66679c763f
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "muef\n"
=======
      "leey\n"
>>>>>>> 2debe65e8c2925068ff9da23ad136e66679c763f
     ]
    }
   ],
   "source": [
    "#user1 = User.objects.create_user('finn', 'mueller-hansen@mcc-berlin.net', 'mcc123')\n",
<<<<<<< HEAD
    "user1, created =  User.objects.get_or_create(username='muef', email='mueller-hansen@mcc-berlin.net')\n",
=======
    "user1, created =  User.objects.get_or_create(username='leey', email='lee@mcc-berlin.net')\n",
>>>>>>> 2debe65e8c2925068ff9da23ad136e66679c763f
    "print(user1)\n",
    "user1.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Note: saving of paragraph and utterance count not working properly, only on the second call of do_search!\n",
    "# pm.Search.objects.all().delete()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 8,
>>>>>>> 2debe65e8c2925068ff9da23ad136e66679c763f
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# number of topics\n",
    "K = 30"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
=======
   "execution_count": 9,
>>>>>>> 2debe65e8c2925068ff9da23ad136e66679c763f
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3284 utterances with search kohle\n",
      "35298\n",
      "3284\n",
<<<<<<< HEAD
      "<QuerySet [<RunStats: RunStats object (950)>, <RunStats: RunStats object (948)>]>\n"
=======
      "None\n",
      "None\n",
      "<QuerySet []>\n"
>>>>>>> 2debe65e8c2925068ff9da23ad136e66679c763f
     ]
    }
   ],
   "source": [
    "# simple search\n",
    "ut_search_tei, created = pm.Search.objects.get_or_create(\n",
    "                title=\"Kohle tei utterance\",\n",
    "                text=\"kohle\",\n",
    "                creator=user1,\n",
    "                document_source=\"GermaParlTEI\",\n",
    "                search_object_type=2)\n",
    "ut_search_tei.save()\n",
    "if created:\n",
    "    do_search(ut_search_tei.id)\n",
    "print(ut_search_tei.par_count)\n",
    "print(ut_search_tei.utterance_count)\n",
    "print(ut_search_tei.runstats_set.all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "creating term frequency-inverse document frequency matrix (4.650237083435059)\n",
      "save terms to db (70.29506087303162)\n",
      "running matrix factorization with NMF (119.18446493148804)\n",
      "Reconstruction error of nmf: 52.49340718950873\n",
      "saving document topic matrix to db (175.3138234615326)\n",
      "topic model run done (183.41522908210754)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
=======
      "creating term frequency-inverse document frequency matrix (5.21019983291626)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-38a0712e260b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# default method is NMF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrun_tm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mut_search_tei\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/tmv/BasicBrowser/venv/lib/python3.6/site-packages/celery/local.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *a, **kw)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_current_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tmv/BasicBrowser/venv/lib/python3.6/site-packages/celery/app/task.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__self__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__self__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/muef/tmv/BasicBrowser/parliament/tasks.py\u001b[0m in \u001b[0;36mrun_tm\u001b[0;34m(s_id, K, language, verbosity, method, max_features, max_df, min_df, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m             )\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0mtfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m         \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tmv/BasicBrowser/venv/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1379\u001b[0m             \u001b[0mTf\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0midf\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mweighted\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mterm\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1380\u001b[0m         \"\"\"\n\u001b[0;32m-> 1381\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1382\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tmv/BasicBrowser/venv/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 869\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tmv/BasicBrowser/venv/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 792\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    793\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tmv/BasicBrowser/venv/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 266\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/muef/tmv/BasicBrowser/utils/text.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstemmer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSnowballStemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"german\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/muef/tmv/BasicBrowser/utils/text.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstemmer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSnowballStemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"german\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tmv/BasicBrowser/venv/lib/python3.6/site-packages/nltk/stem/snowball.py\u001b[0m in \u001b[0;36mstem\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m   1732\u001b[0m                     \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1734\u001b[0;31m         \u001b[0mr1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_r1r2_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__vowels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1736\u001b[0m         \u001b[0;31m# R1 is adjusted so that the region before it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
>>>>>>> 2debe65e8c2925068ff9da23ad136e66679c763f
    }
   ],
   "source": [
    "# default method is NMF\n",
    "run_tm(ut_search_tei.id, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running dynamic NMF algorithm\n",
      "\n",
      "#######################\n",
      "in period 13: 427 docs\n",
      "Extracting tf-idf features for NMF...\n",
      "done in 10.063s.\n",
      "Save terms to DB\n",
      "Fitting the NMF model with tf-idf features, n_samples=1000 and max_features=20000...\n",
      "done in 3.642s.\n",
      "Adding topicterms to db\n",
      "done in 15.243s.\n",
      "\n",
      "#######################\n",
      "in period 14: 507 docs\n",
      "Extracting tf-idf features for NMF...\n",
      "done in 11.252s.\n",
      "Save terms to DB\n",
      "Fitting the NMF model with tf-idf features, n_samples=1000 and max_features=20000...\n",
      "done in 4.119s.\n",
      "Adding topicterms to db\n",
      "done in 17.667s.\n",
      "\n",
      "#######################\n",
      "in period 15: 405 docs\n",
      "Extracting tf-idf features for NMF...\n",
      "done in 8.733s.\n",
      "Save terms to DB\n",
      "Fitting the NMF model with tf-idf features, n_samples=1000 and max_features=20000...\n",
      "done in 3.210s.\n",
      "Adding topicterms to db\n",
      "done in 14.735s.\n",
      "\n",
      "#######################\n",
      "in period 16: 707 docs\n",
      "Extracting tf-idf features for NMF...\n",
      "done in 13.917s.\n",
      "Save terms to DB\n",
      "Fitting the NMF model with tf-idf features, n_samples=1000 and max_features=20000...\n",
      "done in 4.760s.\n",
      "Adding topicterms to db\n",
      "done in 18.750s.\n",
      "\n",
      "#######################\n",
      "in period 17: 651 docs\n",
      "Extracting tf-idf features for NMF...\n",
      "done in 10.864s.\n",
      "Save terms to DB\n",
      "Fitting the NMF model with tf-idf features, n_samples=1000 and max_features=20000...\n",
      "done in 4.012s.\n",
      "Adding topicterms to db\n",
      "done in 16.497s.\n",
      "\n",
      "#######################\n",
      "in period 18: 587 docs\n",
      "Extracting tf-idf features for NMF...\n",
      "done in 9.090s.\n",
      "Save terms to DB\n",
      "Fitting the NMF model with tf-idf features, n_samples=1000 and max_features=20000...\n",
      "done in 3.428s.\n",
      "Adding topicterms to db\n",
      "done in 14.703s.\n",
      "[11345, 11344, 11343, 11342, 11341, 11340, 11339, 11338, 11337, 11336, 11335, 11334, 11333, 11332, 11331, 11330, 11329, 11328, 11327, 11326, 11325, 11324, 11323, 11322, 11321, 11320, 11319, 11318, 11317, 11316]\n",
      "Adding topicterms to db\n",
      "done in 1.197s.\n",
      "951\n",
      "RunStats object (951)\n",
      "done! total time: 4 minutes and 44 seconds\n",
      "a maximum of 713.188 MB was used\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_tm(ut_search_tei.id, 30, method='DT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108750\n",
      "8594\n",
      "<QuerySet [<RunStats: RunStats object (121)>, <RunStats: RunStats object (117)>, <RunStats: RunStats object (116)>, <RunStats: RunStats object (119)>, <RunStats: RunStats object (120)>]>\n"
     ]
    }
   ],
   "source": [
    "# simple search\n",
    "ut_search_pdf, created = pm.Search.objects.get_or_create(\n",
    "                title=\"Kohle pdf utterance all\",\n",
    "                text=\"kohle\",\n",
    "                creator=user1,\n",
    "#                start_date=datetime.date(1996,2,8),\n",
    "#                stop_date=datetime.date(2016,12,16),\n",
    "                document_source=\"from https.*scans of pdfs with xml metadata\",\n",
    "                search_object_type=2)\n",
    "ut_search_pdf.save()\n",
    "if created:\n",
    "    do_search(ut_search_pdf.id)\n",
    "print(ut_search_pdf.par_count)\n",
    "print(ut_search_pdf.utterance_count)\n",
    "print(ut_search_pdf.runstats_set.all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating term frequency-inverse document frequency matrix (7.246098756790161)\n",
      "save terms to db (140.9236991405487)\n",
      "running matrix factorization with NMF (319.11061358451843)\n",
      "Reconstruction error of nmf: 85.31024041679362\n",
      "saving document topic matrix to db (412.3995473384857)\n",
      "topic model run done (425.2567837238312)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_tm(ut_search_pdf.id, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running dynamic NMF algorithm\n",
      "\n",
      "#######################\n",
      "in period 1: 620 docs\n",
      "Extracting tf-idf features for NMF...\n",
      "done in 11.540s.\n",
      "Save terms to DB\n",
      "Fitting the NMF model with tf-idf features, n_samples=1000 and max_features=20000...\n",
      "done in 3.436s.\n",
      "Adding topicterms to db\n",
      "done in 14.120s.\n",
      "\n",
      "#######################\n",
      "in period 2: 315 docs\n",
      "Extracting tf-idf features for NMF...\n",
      "done in 9.854s.\n",
      "Save terms to DB\n",
      "Fitting the NMF model with tf-idf features, n_samples=1000 and max_features=20000...\n",
      "done in 2.769s.\n",
      "Adding topicterms to db\n",
      "done in 12.812s.\n",
      "\n",
      "#######################\n",
      "in period 3: 265 docs\n",
      "Extracting tf-idf features for NMF...\n",
      "done in 5.832s.\n",
      "Save terms to DB\n",
      "Fitting the NMF model with tf-idf features, n_samples=1000 and max_features=20000...\n",
      "done in 1.809s.\n",
      "Adding topicterms to db\n",
      "done in 8.761s.\n",
      "\n",
      "#######################\n",
      "in period 4: 213 docs\n",
      "Extracting tf-idf features for NMF...\n",
      "done in 4.955s.\n",
      "Save terms to DB\n",
      "Fitting the NMF model with tf-idf features, n_samples=1000 and max_features=20000...\n",
      "done in 1.602s.\n",
      "Adding topicterms to db\n",
      "done in 8.266s.\n",
      "\n",
      "#######################\n",
      "in period 5: 404 docs\n",
      "Extracting tf-idf features for NMF...\n",
      "done in 11.780s.\n",
      "Save terms to DB\n",
      "Fitting the NMF model with tf-idf features, n_samples=1000 and max_features=20000...\n",
      "done in 3.629s.\n",
      "Adding topicterms to db\n",
      "done in 13.936s.\n",
      "\n",
      "#######################\n",
      "in period 6: 146 docs\n",
      "Extracting tf-idf features for NMF...\n",
      "done in 4.043s.\n",
      "Save terms to DB\n",
      "Fitting the NMF model with tf-idf features, n_samples=1000 and max_features=20000...\n",
      "done in 1.254s.\n",
      "Adding topicterms to db\n",
      "done in 6.814s.\n",
      "\n",
      "#######################\n",
      "in period 7: 308 docs\n",
      "Extracting tf-idf features for NMF...\n",
      "done in 8.722s.\n",
      "Save terms to DB\n",
      "Fitting the NMF model with tf-idf features, n_samples=1000 and max_features=20000...\n",
      "done in 2.644s.\n",
      "Adding topicterms to db\n",
      "done in 11.273s.\n",
      "\n",
      "#######################\n",
      "in period 8: 473 docs\n",
      "Extracting tf-idf features for NMF...\n",
      "done in 12.456s.\n",
      "Save terms to DB\n",
      "Fitting the NMF model with tf-idf features, n_samples=1000 and max_features=20000...\n",
      "done in 4.342s.\n",
      "Adding topicterms to db\n",
      "done in 16.528s.\n",
      "\n",
      "#######################\n",
      "in period 9: 261 docs\n",
      "Extracting tf-idf features for NMF...\n",
      "done in 4.886s.\n",
      "Save terms to DB\n",
      "Fitting the NMF model with tf-idf features, n_samples=1000 and max_features=20000...\n",
      "done in 1.781s.\n",
      "Adding topicterms to db\n",
      "done in 8.637s.\n",
      "\n",
      "#######################\n",
      "in period 10: 693 docs\n",
      "Extracting tf-idf features for NMF...\n",
      "done in 8.785s.\n",
      "Save terms to DB\n",
      "Fitting the NMF model with tf-idf features, n_samples=1000 and max_features=20000...\n",
      "done in 3.883s.\n",
      "Adding topicterms to db\n",
      "done in 14.969s.\n",
      "\n",
      "#######################\n",
      "in period 11: 828 docs\n",
      "Extracting tf-idf features for NMF...\n",
      "done in 11.671s.\n",
      "Save terms to DB\n",
      "Fitting the NMF model with tf-idf features, n_samples=1000 and max_features=20000...\n",
      "done in 4.035s.\n",
      "Adding topicterms to db\n",
      "done in 15.714s.\n",
      "\n",
      "#######################\n",
      "in period 12: 637 docs\n",
      "Extracting tf-idf features for NMF...\n",
      "done in 8.652s.\n",
      "Save terms to DB\n",
      "Fitting the NMF model with tf-idf features, n_samples=1000 and max_features=20000...\n",
      "done in 3.286s.\n",
      "Adding topicterms to db\n",
      "done in 13.818s.\n",
      "\n",
      "#######################\n",
      "in period 13: 616 docs\n",
      "Extracting tf-idf features for NMF...\n",
      "done in 8.274s.\n",
      "Save terms to DB\n",
      "Fitting the NMF model with tf-idf features, n_samples=1000 and max_features=20000...\n",
      "done in 3.296s.\n",
      "Adding topicterms to db\n",
      "done in 13.243s.\n",
      "\n",
      "#######################\n",
      "in period 14: 484 docs\n",
      "Extracting tf-idf features for NMF...\n",
      "done in 5.647s.\n",
      "Save terms to DB\n",
      "Fitting the NMF model with tf-idf features, n_samples=1000 and max_features=20000...\n",
      "done in 2.238s.\n",
      "Adding topicterms to db\n",
      "done in 10.280s.\n",
      "\n",
      "#######################\n",
      "in period 15: 395 docs\n",
      "Extracting tf-idf features for NMF...\n",
      "done in 4.777s.\n",
      "Save terms to DB\n",
      "Fitting the NMF model with tf-idf features, n_samples=1000 and max_features=20000...\n",
      "done in 1.874s.\n",
      "Adding topicterms to db\n",
      "done in 10.278s.\n",
      "\n",
      "#######################\n",
      "in period 16: 658 docs\n",
      "Extracting tf-idf features for NMF...\n",
      "done in 6.714s.\n",
      "Save terms to DB\n",
      "Fitting the NMF model with tf-idf features, n_samples=1000 and max_features=20000...\n",
      "done in 2.560s.\n",
      "Adding topicterms to db\n",
      "done in 11.169s.\n",
      "\n",
      "#######################\n",
      "in period 17: 634 docs\n",
      "Extracting tf-idf features for NMF...\n",
      "done in 5.954s.\n",
      "Save terms to DB\n",
      "Fitting the NMF model with tf-idf features, n_samples=1000 and max_features=20000...\n",
      "done in 2.560s.\n",
      "Adding topicterms to db\n",
      "done in 10.580s.\n",
      "\n",
      "#######################\n",
      "in period 18: 644 docs\n",
      "Extracting tf-idf features for NMF...\n",
      "done in 5.590s.\n",
      "Save terms to DB\n",
      "Fitting the NMF model with tf-idf features, n_samples=1000 and max_features=20000...\n",
      "done in 2.190s.\n",
      "Adding topicterms to db\n",
      "done in 9.438s.\n",
      "<QuerySet [<Topic: Topic 7>, <Topic: Topic 8>, <Topic: Topic 10>, <Topic: Topic 11>, <Topic: Topic 12>, <Topic: Topic 14>, <Topic: Topic 15>, <Topic: Topic 16>, <Topic: Topic 17>, <Topic: Topic 18>, <Topic: Topic 19>, <Topic: Topic 20>, <Topic: Topic 2>, <Topic: Topic 3>, <Topic: Topic 4>, <Topic: Topic 6>, <Topic: Topic 7>, <Topic: Topic 9>, <Topic: Topic 10>, <Topic: Topic 11>, '...(remaining elements truncated)...']>\n",
      "[481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510]\n",
      "Adding topicterms to db\n",
      "done in 1.024s.\n",
      "saving primary topic not working\n",
      "117\n",
      "RunStats object (117)\n",
      "done! total time: 24 minutes and 55 seconds\n",
      "a maximum of 1220.048 MB was used\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_tm(ut_search_pdf.id, K, method='DT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Blei DTM algorithm\n",
      "Extracting word features...\n",
      "done in 119.577s.\n",
      "Save terms to DB\n",
      "write input files for Blei algorithm\n",
      "Calling Blei algorithm\n",
      "Blei algorithm done\n",
      "upload dtm results to db\n",
      "writing topic terms\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "writing doctopics\n",
      "119\n",
      "RunStats object (119)\n",
      "done! total time: 268 minutes and 34 seconds\n",
      "a maximum of 945.744 MB was used\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_tm(ut_search_pdf.id, 30, method='BT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Blei DTM algorithm\n",
      "Extracting word features...\n",
      "done in 124.474s.\n",
      "Save terms to DB\n",
      "write input files for Blei algorithm\n",
      "Calling Blei algorithm\n",
      "Blei algorithm done\n",
      "upload dtm results to db\n",
      "writing topic terms\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "14\n",
      "13\n",
      "7\n",
      "15\n",
      "3\n",
      "5\n",
      "9\n",
      "11\n",
      "1\n",
      "16\n",
      "18\n",
      "20\n",
      "22\n",
      "24\n",
      "26\n",
      "28\n",
      "30\n",
      "17\n",
      "19\n",
      "25\n",
      "21\n",
      "23\n",
      "27\n",
      "29\n",
      "32\n",
      "34\n",
      "31\n",
      "36\n",
      "38\n",
      "40\n",
      "42\n",
      "44\n",
      "33\n",
      "46\n",
      "39\n",
      "37\n",
      "41\n",
      "35\n",
      "43\n",
      "45\n",
      "48\n",
      "47\n",
      "49\n",
      "writing doctopics\n",
      "122\n",
      "RunStats object (122)\n",
      "done! total time: 410 minutes and 33 seconds\n",
      "a maximum of 778.852 MB was used\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_tm(ut_search_pdf.id, 50, method='BT')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tmv_yt",
   "language": "python",
   "name": "tmv_yt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
